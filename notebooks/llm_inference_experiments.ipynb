{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b8809-84e6-40e5-bf1e-dcb9d791ca44",
   "metadata": {},
   "source": [
    "### Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b5f003-4633-432e-aea6-6be5a22b706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c040a-f95f-4bea-9406-65634f1d57fd",
   "metadata": {},
   "source": [
    "### Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2de914-bf89-4693-90ad-2ea6e9734aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../secrets/credentials.yml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "OPENAI_CREDENTIALS = config['OPENAI_CREDENTIALS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf30bb3-0d23-4a9b-9547-43efd8213fc1",
   "metadata": {},
   "source": [
    "### Function to inference the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d832db5f-7ec7-4603-a0e2-c29aaaa15380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(api_key, model, prompt_text, max_completion_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Queries an OpenAI chat model with customizable parameters and returns the response or a structured error message.\n",
    "\n",
    "    :param api_key: Your OpenAI API key.\n",
    "    :param model: The chat model to query (e.g., \"gpt-3.5-turbo\").\n",
    "    :param prompt_text: The text prompt to send to the model.\n",
    "    :param max_completion_tokens: The maximum number of tokens to generate in the completion.\n",
    "    :param temperature: Controls randomness in the generation. Lower values make the model more deterministic.\n",
    "    :return: The model's response or an error message.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"  \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        \"max_tokens\": max_completion_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Extracting information from the response\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            return (content, usage)\n",
    "        \n",
    "        else:\n",
    "            error_message = f\"HTTP Error {response.status_code}\"\n",
    "            try:\n",
    "                error_details = response.json().get('error', {})\n",
    "                message = error_details.get('message', 'An unspecified error occurred')\n",
    "            except ValueError:\n",
    "                message = \"Error details unavailable\"\n",
    "            error_message += f\": {message}\"\n",
    "\n",
    "            return {\"error\": error_message}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": f\"Connection error: {e}\"}\n",
    "\n",
    "\n",
    "def calculate_inference_price(query_results, input_token_price, output_token_price):\n",
    "    \"\"\"\n",
    "    Calculates the inference cost based on input and output token prices.\n",
    "\n",
    "    Parameters:\n",
    "    - query_results: A dictionary containing \"prompt_tokens\" and \"completion_tokens\" used in a query.\n",
    "    - input_token_price: Price per input token.\n",
    "    - output_token_price: Price per output token.\n",
    "\n",
    "    Returns:\n",
    "    - The total price for the inference based on the provided prices.\n",
    "    \"\"\"\n",
    "    total_price = (query_results[\"prompt_tokens\"] * input_token_price) + \\\n",
    "                  (query_results[\"completion_tokens\"] * output_token_price)\n",
    "    return total_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9f08d-834e-4030-96af-4abe7017139f",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "##### Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40dab0f-ad5e-4178-84c5-a42aad575b7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "input_token_price = 0.0000005\n",
    "output_token_price = 0.0000015\n",
    "max_completion_tokens = 1000  \n",
    "temperature = 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c843022-6c8d-4f31-a371-d8db531651e6",
   "metadata": {},
   "source": [
    "##### Define the prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7547d65f-cc3f-4fe0-92bc-0dd8238b0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following French text to Arabic: \n",
    "Saad LABRIJI\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2384f06-758d-421a-8b31-3fbcc581f905",
   "metadata": {},
   "source": [
    "##### Query OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c979635-344d-4425-9f0b-283d4dd9f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price in 2.3500000000000002e-05$\n",
      "-------------\n",
      "سعد لبريجي\n"
     ]
    }
   ],
   "source": [
    "response = query_openai(OPENAI_CREDENTIALS, model, prompt, max_completion_tokens, temperature)\n",
    "\n",
    "print(f\"Price in {calculate_inference_price(response[1], input_token_price, output_token_price)}$\")\n",
    "print(\"-------------\")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f206f20-0ced-4a38-b701-53a214bce411",
   "metadata": {},
   "source": [
    "#### Function to prepare the Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c01e57-8f2d-449a-a491-4eda78bdbeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert in textile design, your insights are invaluable. Your expertise is needed to address the following challenge:\n",
      "\n",
      "Context: What are the best materials and practices for creating an eco-friendly rag?\n",
      "\n",
      "Consider the following points when formulating your response:\n",
      "- Provide detailed suggestions and explanations.\n",
      "- Include examples and practical tips.\n",
      "- Explore innovative and eco-friendly approaches.\n",
      "\n",
      "Your comprehensive answer will contribute significantly to solving this challenge.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_contextual_rag_prompt(expertise_area, context):\n",
    "    \"\"\"\n",
    "    Generates a detailed and engaging prompt tailored for a specific expertise area and context.\n",
    "\n",
    "    :param expertise_area: The area of expertise to mention in the prompt (e.g., \"textile design\").\n",
    "    :param context: The specific context or question to address (e.g., \"I want to create a sustainable rag\").\n",
    "    :return: The crafted prompt string.\n",
    "    \"\"\"\n",
    "    # Introduction with a call to action\n",
    "    prompt = f\"As an expert in {expertise_area}, your insights are invaluable. Your expertise is needed to address the following challenge:\\n\\n\"\n",
    "\n",
    "    # Setting the context\n",
    "    prompt += f\"Context: {context}\\n\\n\"\n",
    "\n",
    "    # Providing additional guidance and encouragement\n",
    "    prompt += \"Consider the following points when formulating your response:\\n\"\n",
    "    prompt += \"- Provide detailed suggestions and explanations.\\n\"\n",
    "    prompt += \"- Include examples and practical tips.\\n\"\n",
    "    prompt += \"- Explore innovative and eco-friendly approaches.\\n\\n\"\n",
    "\n",
    "    # Prompting for a comprehensive and insightful response\n",
    "    prompt += \"Your comprehensive answer will contribute significantly to solving this challenge.\\n\\n\"\n",
    "\n",
    "    # Encouragement to begin crafting the response\n",
    "    prompt += \"Start your response below:\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "expertise_area = \"textile design\"\n",
    "context = \"What are the best materials and practices for creating an eco-friendly rag?\"\n",
    "\n",
    "result = generate_contextual_rag_prompt(expertise_area, context)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467aa3a-5694-4936-bdf5-423b6c51bbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52e9fd-53a7-46fb-a313-2d4ff22ec0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f0b9a-ee0c-4e68-b98b-a389bc47640d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca04c4-749b-4ea0-8c5c-b7be559a441d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925c4c0-7686-473f-9b10-7f96935cf2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0141b-6bea-4746-ba2e-5b077f01ddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3b2ca-1676-48dd-89c9-46526c512ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

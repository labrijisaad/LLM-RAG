{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac67279-b700-425c-bfc0-7242488b8e6a",
   "metadata": {},
   "source": [
    "## <center><a><span style=\"color:red\">`OpenAI LLM` - Inference Experiments</span></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b8809-84e6-40e5-bf1e-dcb9d791ca44",
   "metadata": {},
   "source": [
    "### Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b5f003-4633-432e-aea6-6be5a22b706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c040a-f95f-4bea-9406-65634f1d57fd",
   "metadata": {},
   "source": [
    "### Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2de914-bf89-4693-90ad-2ea6e9734aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../secrets/credentials.yml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "OPENAI_CREDENTIALS = config['OPENAI_CREDENTIALS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4792ab-46bc-4de9-a86c-6a6c406ff681",
   "metadata": {},
   "source": [
    "## <a><span style=\"color:green\">Query `OpenAI API`</span></a>\n",
    "### Function to inference the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d832db5f-7ec7-4603-a0e2-c29aaaa15380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(api_key, model, prompt_text, max_completion_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Queries an OpenAI chat model with customizable parameters and returns the response or a structured error message.\n",
    "\n",
    "    :param api_key: OpenAI API key.\n",
    "    :param model: Model to query (e.g., \"gpt-3.5-turbo\").\n",
    "    :param prompt_text: Text prompt to send to the model.\n",
    "    :param max_completion_tokens: Maximum number of tokens to generate in the completion.\n",
    "    :param temperature: Controls randomness in the generation. Lower values make the model more deterministic.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"  \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        \"max_tokens\": max_completion_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Extracting information from the response\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            return (content, usage)\n",
    "        \n",
    "        else:\n",
    "            error_message = f\"HTTP Error {response.status_code}\"\n",
    "            try:\n",
    "                error_details = response.json().get('error', {})\n",
    "                message = error_details.get('message', 'An unspecified error occurred')\n",
    "            except ValueError:\n",
    "                message = \"Error details unavailable\"\n",
    "            error_message += f\": {message}\"\n",
    "\n",
    "            return {\"error\": error_message}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": f\"Connection error: {e}\"}\n",
    "\n",
    "\n",
    "def calculate_inference_price(query_results, input_token_price, output_token_price):\n",
    "    \"\"\"\n",
    "    Calculates the inference cost based on input and output token prices.\n",
    "    \"\"\"\n",
    "    total_price = (query_results[\"prompt_tokens\"] * input_token_price) + \\\n",
    "                  (query_results[\"completion_tokens\"] * output_token_price)\n",
    "    return total_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9f08d-834e-4030-96af-4abe7017139f",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "##### Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40dab0f-ad5e-4178-84c5-a42aad575b7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "input_token_price = 0.0000005\n",
    "output_token_price = 0.0000015\n",
    "max_completion_tokens = 1000  \n",
    "temperature = 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c843022-6c8d-4f31-a371-d8db531651e6",
   "metadata": {},
   "source": [
    "##### Define the prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7547d65f-cc3f-4fe0-92bc-0dd8238b0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following French text to Arabic: \n",
    "Saad LABRIJI\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2384f06-758d-421a-8b31-3fbcc581f905",
   "metadata": {},
   "source": [
    "##### Query OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c979635-344d-4425-9f0b-283d4dd9f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: $2.3500000000000002e-05\n",
      "-------------\n",
      "ÿ≥ÿπÿØ ŸÑÿ®ÿ±Ÿäÿ¨Ÿä\n"
     ]
    }
   ],
   "source": [
    "response = query_openai(OPENAI_CREDENTIALS, model, prompt, max_completion_tokens, temperature)\n",
    "\n",
    "print(f\"Price: ${calculate_inference_price(response[1], input_token_price, output_token_price)}\")\n",
    "print(\"-------------\")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6b725-afb7-4a15-8d6b-50aa09900e71",
   "metadata": {},
   "source": [
    "## <a><span style=\"color:green\">`Prompt Preparation` - expertise area detection</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6928ab2-eacf-4ca4-9056-0a6bf7638c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_expertise_area(api_key, user_question, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Queries OpenAI to determine the expertise area(s) relevant to a user's question. The function formats the request to\n",
    "    \"\"\"\n",
    "    # Prepare the prompt\n",
    "    prompt_text = f\"\"\"Based on the question provided, identify the relevant expertise area(s). Return your answer in the format: \n",
    "    'expertise1, expertise2, ...'. Provide only the expertise areas as a comma-separated list, no additional explanations are needed.\n",
    "    Here is the user Question:\n",
    "    {user_question}\n",
    "    \"\"\"\n",
    "    response, usage = query_openai(api_key, model, prompt_text, max_completion_tokens=100, temperature=0.3)\n",
    "    cleaned_response = response.strip()    \n",
    "    return cleaned_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af878e0-931d-4477-9871-569dacc82b3a",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19564d7-bd89-401d-b3b0-ca19d5c22455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineering, Apache Airflow\n"
     ]
    }
   ],
   "source": [
    "user_question = \"help me understand the Airflow concept in my files.\"\n",
    "expertise_area = determine_expertise_area(OPENAI_CREDENTIALS, user_question)\n",
    "print(expertise_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ecf07-1174-4338-957f-902154ff0f29",
   "metadata": {},
   "source": [
    "## <a><span style=\"color:green\">`Prompt Preparation` - contextual prompt preparation</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c96a3c8-9b73-4cf9-b204-f07d017aa6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_for_llm(expertise_area, user_question, context_documents):\n",
    "    \"\"\"\n",
    "    Prepares a detailed and engaging prompt tailored for a specific expertise area, user question, and context documents.\n",
    "    \"\"\"\n",
    "    # Introduction with a specific call to action based on expertise\n",
    "    prompt = (\n",
    "        f\"You are an expert in '{expertise_area}'. A user has asked for help with the following question: \"\n",
    "        f\"'{user_question}'. Please provide insights using only the information from the provided documents. \"\n",
    "        \"If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Injecting the context by appending documents\n",
    "    prompt += \"Here are the documents provided:\\n\\n\"\n",
    "    for i, document in enumerate(context_documents, start=1):\n",
    "        prompt += f\"Document {i}:\\n\\\"\\\"\\\"\\n{document}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "    \n",
    "    # Prompting for a comprehensive and insightful response\n",
    "    prompt += (\n",
    "        \"Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Encouragement to begin crafting the response\n",
    "    prompt += \"Start your response below:\\n\\n\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb89253-f2e8-422e-9636-c78e69544e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in 'Natural Language Processing, Information Retrieval'. A user has asked for help with the following question: 'What is retrieval-augmented generation?'. Please provide insights using only the information from the provided documents. If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\n",
      "\n",
      "Here are the documents provided:\n",
      "\n",
      "Document 1:\n",
      "\"\"\"\n",
      "Document 1 content about RAG...\n",
      "\"\"\"\n",
      "\n",
      "Document 2:\n",
      "\"\"\"\n",
      "Document 2 content about LLMs & RAGs...\n",
      "\"\"\"\n",
      "\n",
      "Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_question = \"What is retrieval-augmented generation?\"\n",
    "\n",
    "# Simulate docs\n",
    "context_documents = [\n",
    "    \"Document 1 content about RAG...\",\n",
    "    \"Document 2 content about LLMs & RAGs...\",\n",
    "]\n",
    "\n",
    "# Detremine the expertise area\n",
    "expertise_area = determine_expertise_area(OPENAI_CREDENTIALS, user_question)\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = prepare_prompt_for_llm(expertise_area, user_question, context_documents)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723554e8-5cb2-4a89-ba48-b7e295913197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791c101-e95b-43e0-a03a-72efcadd7228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f3d64-3107-4064-802b-3eee787757ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726b463-fdbe-4ab7-9f47-892d073af432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018780a4-1ff5-4fb8-952f-331836126e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52e9fd-53a7-46fb-a313-2d4ff22ec0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f0b9a-ee0c-4e68-b98b-a389bc47640d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca04c4-749b-4ea0-8c5c-b7be559a441d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925c4c0-7686-473f-9b10-7f96935cf2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0141b-6bea-4746-ba2e-5b077f01ddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18d5466-5b46-4b1c-9074-bd8f1a9cd328",
   "metadata": {},
   "source": [
    "## Connect with me üåê\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://www.linkedin.com/in/labrijisaad/\">\n",
    "    <img src=\"https://img.shields.io/badge/LinkedIn-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn\" style=\"margin-bottom: 5px;\"/>\n",
    "  </a>\n",
    "  <a href=\"https://github.com/labrijisaad\">\n",
    "    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub\" style=\"margin-bottom: 5px;\"/>\n",
    "  </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

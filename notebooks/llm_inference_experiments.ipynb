{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b8809-84e6-40e5-bf1e-dcb9d791ca44",
   "metadata": {},
   "source": [
    "#### Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b5f003-4633-432e-aea6-6be5a22b706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c040a-f95f-4bea-9406-65634f1d57fd",
   "metadata": {},
   "source": [
    "#### Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2de914-bf89-4693-90ad-2ea6e9734aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../secrets/credentials.yml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "OPENAI_CREDENTIALS = config['OPENAI_CREDENTIALS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf30bb3-0d23-4a9b-9547-43efd8213fc1",
   "metadata": {},
   "source": [
    "#### Function to inference the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d832db5f-7ec7-4603-a0e2-c29aaaa15380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai_model(api_key, model, prompt_text, max_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Queries an OpenAI chat model with customizable parameters and returns the response or a structured error message.\n",
    "\n",
    "    :param api_key: Your OpenAI API key.\n",
    "    :param model: The chat model to query (e.g., \"gpt-3.5-turbo\").\n",
    "    :param prompt_text: The text prompt to send to the model.\n",
    "    :param max_tokens: The maximum number of tokens to generate in the completion.\n",
    "    :param temperature: Controls randomness in the generation. Lower values make the model more deterministic.\n",
    "    :return: The model's response or an error message.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"  \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Extracting information from the response\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            return (content, usage)\n",
    "        \n",
    "        else:\n",
    "            error_message = f\"HTTP Error {response.status_code}\"\n",
    "            try:\n",
    "                error_details = response.json().get('error', {})\n",
    "                message = error_details.get('message', 'An unspecified error occurred')\n",
    "            except ValueError:\n",
    "                message = \"Error details unavailable\"\n",
    "            error_message += f\": {message}\"\n",
    "\n",
    "            return {\"error\": error_message}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": f\"Connection error: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9f08d-834e-4030-96af-4abe7017139f",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb57a7f-afc0-4ba4-a60d-66608e61a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, le monde! Je suis Saad\n",
      "{'prompt_tokens': 25, 'completion_tokens': 9, 'total_tokens': 34}\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "prompt = \"Translate the following English text to French: 'Hello, world! I am Saad'\"\n",
    "max_tokens = 150  \n",
    "temperature = 0.5 \n",
    "\n",
    "response = query_openai_model(OPENAI_CREDENTIALS, model, prompt, max_tokens, temperature)\n",
    "\n",
    "print(response[0])\n",
    "print(response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f206f20-0ced-4a38-b701-53a214bce411",
   "metadata": {},
   "source": [
    "#### Function to prepare the Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c01e57-8f2d-449a-a491-4eda78bdbeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert in textile design, your insights are invaluable. Your expertise is needed to address the following challenge:\n",
      "\n",
      "Context: What are the best materials and practices for creating an eco-friendly rag?\n",
      "\n",
      "Consider the following points when formulating your response:\n",
      "- Provide detailed suggestions and explanations.\n",
      "- Include examples and practical tips.\n",
      "- Explore innovative and eco-friendly approaches.\n",
      "\n",
      "Your comprehensive answer will contribute significantly to solving this challenge.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_contextual_rag_prompt(expertise_area, context):\n",
    "    \"\"\"\n",
    "    Generates a detailed and engaging prompt tailored for a specific expertise area and context.\n",
    "\n",
    "    :param expertise_area: The area of expertise to mention in the prompt (e.g., \"textile design\").\n",
    "    :param context: The specific context or question to address (e.g., \"I want to create a sustainable rag\").\n",
    "    :return: The crafted prompt string.\n",
    "    \"\"\"\n",
    "    # Introduction with a call to action\n",
    "    prompt = f\"As an expert in {expertise_area}, your insights are invaluable. Your expertise is needed to address the following challenge:\\n\\n\"\n",
    "\n",
    "    # Setting the context\n",
    "    prompt += f\"Context: {context}\\n\\n\"\n",
    "\n",
    "    # Providing additional guidance and encouragement\n",
    "    prompt += \"Consider the following points when formulating your response:\\n\"\n",
    "    prompt += \"- Provide detailed suggestions and explanations.\\n\"\n",
    "    prompt += \"- Include examples and practical tips.\\n\"\n",
    "    prompt += \"- Explore innovative and eco-friendly approaches.\\n\\n\"\n",
    "\n",
    "    # Prompting for a comprehensive and insightful response\n",
    "    prompt += \"Your comprehensive answer will contribute significantly to solving this challenge.\\n\\n\"\n",
    "\n",
    "    # Encouragement to begin crafting the response\n",
    "    prompt += \"Start your response below:\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "expertise_area = \"textile design\"\n",
    "context = \"What are the best materials and practices for creating an eco-friendly rag?\"\n",
    "\n",
    "result = generate_contextual_rag_prompt(expertise_area, context)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

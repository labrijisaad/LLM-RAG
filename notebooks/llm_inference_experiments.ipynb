{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bda620-3817-456d-bbb1-74afb403be76",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/labrijisaad/LLM-RAG/blob/main/notebooks/llm_inference_experiments.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://img.shields.io/badge/Open%20in-GitHub-blue.svg\" alt=\"Open In GitHub\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac67279-b700-425c-bfc0-7242488b8e6a",
   "metadata": {},
   "source": [
    "## <center><a><span style=\"color:red\">`OpenAI LLM` - Inference Experiments</span></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b8809-84e6-40e5-bf1e-dcb9d791ca44",
   "metadata": {},
   "source": [
    "### Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b5f003-4633-432e-aea6-6be5a22b706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c040a-f95f-4bea-9406-65634f1d57fd",
   "metadata": {},
   "source": [
    "### Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2de914-bf89-4693-90ad-2ea6e9734aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../secrets/credentials.yml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "OPENAI_CREDENTIALS = config[\"OPENAI_CREDENTIALS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4792ab-46bc-4de9-a86c-6a6c406ff681",
   "metadata": {},
   "source": [
    "## <a><span style=\"color:green\">Query `OpenAI API`</span></a>\n",
    "### Function to inference the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d832db5f-7ec7-4603-a0e2-c29aaaa15380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(\n",
    "    api_key, model, prompt_text, max_completion_tokens=100, temperature=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Queries an OpenAI chat model with customizable parameters and returns the response or a structured error message.\n",
    "\n",
    "    :param api_key: OpenAI API key.\n",
    "    :param model: Model to query (e.g., \"gpt-3.5-turbo\").\n",
    "    :param prompt_text: Text prompt to send to the model.\n",
    "    :param max_completion_tokens: Maximum number of tokens to generate in the completion.\n",
    "    :param temperature: Controls randomness in the generation. Lower values make the model more deterministic.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        \"max_tokens\": max_completion_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Extracting information from the response\n",
    "            content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            usage = data[\"usage\"]\n",
    "            return (content, usage)\n",
    "\n",
    "        else:\n",
    "            error_message = f\"HTTP Error {response.status_code}\"\n",
    "            try:\n",
    "                error_details = response.json().get(\"error\", {})\n",
    "                message = error_details.get(\"message\", \"An unspecified error occurred\")\n",
    "            except ValueError:\n",
    "                message = \"Error details unavailable\"\n",
    "            error_message += f\": {message}\"\n",
    "\n",
    "            return {\"error\": error_message}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": f\"Connection error: {e}\"}\n",
    "\n",
    "\n",
    "def calculate_inference_price(query_results, input_token_price, output_token_price):\n",
    "    \"\"\"\n",
    "    Calculates the inference cost based on input and output token prices.\n",
    "    \"\"\"\n",
    "    total_price = (query_results[\"prompt_tokens\"] * input_token_price) + (\n",
    "        query_results[\"completion_tokens\"] * output_token_price\n",
    "    )\n",
    "    return total_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9f08d-834e-4030-96af-4abe7017139f",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "##### Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40dab0f-ad5e-4178-84c5-a42aad575b7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "input_token_price = 0.0000005\n",
    "output_token_price = 0.0000015\n",
    "max_completion_tokens = 1000\n",
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c843022-6c8d-4f31-a371-d8db531651e6",
   "metadata": {},
   "source": [
    "##### Define the prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7547d65f-cc3f-4fe0-92bc-0dd8238b0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following French text to Arabic: \n",
    "Saad LABRIJI\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2384f06-758d-421a-8b31-3fbcc581f905",
   "metadata": {},
   "source": [
    "##### Query OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c979635-344d-4425-9f0b-283d4dd9f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: $2.3500000000000002e-05\n",
      "-------------\n",
      "ÿ≥ÿπÿØ ŸÑÿ®ÿ±Ÿäÿ¨Ÿä\n"
     ]
    }
   ],
   "source": [
    "response = query_openai(\n",
    "    OPENAI_CREDENTIALS, model, prompt, max_completion_tokens, temperature\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Price: ${calculate_inference_price(response[1], input_token_price, output_token_price)}\"\n",
    ")\n",
    "print(\"-------------\")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6b725-afb7-4a15-8d6b-50aa09900e71",
   "metadata": {},
   "source": [
    "## <a><span style=\"color:green\">`Prompt Preparation` - expertise area detection</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6928ab2-eacf-4ca4-9056-0a6bf7638c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_expertise_area(api_key, user_question, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Queries OpenAI to determine the expertise area(s) relevant to a user's question. The function formats the request to\n",
    "    \"\"\"\n",
    "    # Prepare the prompt\n",
    "    prompt_text = f\"\"\"Based on the question provided, identify the relevant expertise area(s). Return your answer in the format: \n",
    "    'expertise1, expertise2, ...'. Provide only the expertise areas as a comma-separated list, no additional explanations are needed.\n",
    "    Here is the user Question:\n",
    "    {user_question}\n",
    "    \"\"\"\n",
    "    response, usage = query_openai(\n",
    "        api_key, model, prompt_text, max_completion_tokens=100, temperature=0.3\n",
    "    )\n",
    "    cleaned_response = response.strip()\n",
    "    return cleaned_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af878e0-931d-4477-9871-569dacc82b3a",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19564d7-bd89-401d-b3b0-ca19d5c22455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineering, Workflow Automation, Apache Airflow\n"
     ]
    }
   ],
   "source": [
    "user_question = \"help me understand the Airflow concept in my files.\"\n",
    "expertise_area = determine_expertise_area(OPENAI_CREDENTIALS, user_question)\n",
    "print(expertise_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ecf07-1174-4338-957f-902154ff0f29",
   "metadata": {},
   "source": [
    "## <a><span style=\"color:green\">`Prompt Preparation` - contextual prompt preparation</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c96a3c8-9b73-4cf9-b204-f07d017aa6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_for_llm(expertise_area, user_question, context_documents):\n",
    "    \"\"\"\n",
    "    Prepares a detailed and engaging prompt tailored for a specific expertise area, user question, and context documents.\n",
    "    \"\"\"\n",
    "    # Introduction with a specific call to action based on expertise\n",
    "    prompt = (\n",
    "        f\"You are an expert in '{expertise_area}'. A user has asked for help with the following question: \"\n",
    "        f\"'{user_question}'. Please provide insights using only the information from the provided documents. \"\n",
    "        \"If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Injecting the context by appending documents\n",
    "    prompt += \"Here are the documents provided:\\n\\n\"\n",
    "    for i, document in enumerate(context_documents, start=1):\n",
    "        prompt += f'Document {i}:\\n\"\"\"\\n{document}\\n\"\"\"\\n\\n'\n",
    "\n",
    "    # Prompting for a comprehensive and insightful response\n",
    "    prompt += \"Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\\n\\n\"\n",
    "\n",
    "    # Encouragement to begin crafting the response\n",
    "    prompt += \"Start your response below:\\n\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb89253-f2e8-422e-9636-c78e69544e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in 'Natural Language Processing, Information Retrieval'. A user has asked for help with the following question: 'What is retrieval-augmented generation?'. Please provide insights using only the information from the provided documents. If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\n",
      "\n",
      "Here are the documents provided:\n",
      "\n",
      "Document 1:\n",
      "\"\"\"\n",
      "Document 1 content about RAG...\n",
      "\"\"\"\n",
      "\n",
      "Document 2:\n",
      "\"\"\"\n",
      "Document 2 content about LLMs & RAGs...\n",
      "\"\"\"\n",
      "\n",
      "Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_question = \"What is retrieval-augmented generation?\"\n",
    "\n",
    "# Simulate docs\n",
    "context_documents = [\n",
    "    \"Document 1 content about RAG...\",\n",
    "    \"Document 2 content about LLMs & RAGs...\",\n",
    "]\n",
    "\n",
    "# Detremine the expertise area\n",
    "expertise_area = determine_expertise_area(OPENAI_CREDENTIALS, user_question)\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = prepare_prompt_for_llm(expertise_area, user_question, context_documents)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fc843-38b7-4b79-9d7d-29a35d5d1995",
   "metadata": {},
   "source": [
    "## <center><a><span style=\"color:red\">`OpenAI LLM` - Inference Experiments - `Class Based`</span></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa40d88f-d0c3-49b1-86b8-7dd73ca73040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class PromptPreparator:\n",
    "    def __init__(self, api_key, models_config):\n",
    "        self.api_key = api_key\n",
    "        self.models_config = models_config\n",
    "        self.model = None\n",
    "        self.input_token_price = 0\n",
    "        self.output_token_price = 0\n",
    "\n",
    "    def set_model(self, model_name):\n",
    "        for group in self.models_config[\"models\"]:\n",
    "            for variant in group[\"variants\"]:\n",
    "                if variant[\"model\"] == model_name:\n",
    "                    self.model = model_name\n",
    "                    self.input_token_price = variant.get(\"input_price_per_token\", 0)\n",
    "                    self.output_token_price = variant.get(\"output_price_per_token\", 0)\n",
    "                    return\n",
    "        raise ValueError(f\"Model {model_name} not found in configuration.\")\n",
    "\n",
    "    def query_openai(self, prompt_text, max_completion_tokens=100, temperature=0.7):\n",
    "        if not self.model:\n",
    "            raise ValueError(\n",
    "                \"Model not set. Please use set_model() to set a model before querying.\"\n",
    "            )\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            \"max_tokens\": max_completion_tokens,\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                usage = data[\"usage\"]\n",
    "                return content, usage\n",
    "            else:\n",
    "                return (\n",
    "                    f\"HTTP Error {response.status_code}: {response.json().get('error', {}).get('message', 'An unspecified error occurred')}\",\n",
    "                    None,\n",
    "                )\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Connection error: {e}\", None\n",
    "\n",
    "    def calculate_inference_price(self, usage):\n",
    "        if usage:\n",
    "            total_price = (usage[\"prompt_tokens\"] * self.input_token_price) + (\n",
    "                usage[\"completion_tokens\"] * self.output_token_price\n",
    "            )\n",
    "            return total_price\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def determine_expertise_area(\n",
    "        self, user_question, max_completion_tokens=150, temperature=0.2\n",
    "    ):\n",
    "        prompt_text = f\"\"\"Based on the question provided, identify the relevant expertise area(s). Return your answer in the format: \n",
    "        'expertise1, expertise2, ...'. Provide only the expertise areas as a comma-separated list, no additional explanations are needed.\n",
    "        Here is the user Question:\n",
    "        {user_question}\n",
    "        \"\"\"\n",
    "        response, usage = self.query_openai(\n",
    "            prompt_text, max_completion_tokens, temperature\n",
    "        )\n",
    "        return response.strip(), (\n",
    "            usage if response else \"Error determining expertise area.\"\n",
    "        )\n",
    "\n",
    "    def prepare_prompt_for_llm(self, expertise_area, user_question, context_documents):\n",
    "        prompt = (\n",
    "            f\"You are an expert in '{expertise_area}'. A user has asked for help with the following question: \"\n",
    "            f\"'{user_question}'. Please provide insights using only the information from the provided documents. \"\n",
    "            \"If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\\n\\n\"\n",
    "            \"Here are the documents provided:\\n\\n\"\n",
    "        )\n",
    "        for i, document in enumerate(context_documents, start=1):\n",
    "            prompt += f'Document {i}:\\n\"\"\"\\n{document}\\n\"\"\"\\n\\n'\n",
    "        prompt += \"Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\\n\\n\"\n",
    "        prompt += \"Start your response below:\\n\\n\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00626dd3-4466-40c4-88e8-14dcd2a0281f",
   "metadata": {},
   "source": [
    "#### Load OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7fc411-e73c-4e7f-8338-d3dcf0506203",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../secrets/credentials.yml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "OPENAI_CREDENTIALS = config[\"OPENAI_CREDENTIALS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff025c-073f-4706-9b5a-bd2f577260ed",
   "metadata": {},
   "source": [
    "#### Load Model Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc39551-75aa-4c3e-a9ab-d2b208e00b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'GPT-4 Turbo',\n",
       "   'variants': [{'model': 'gpt-4-0125-preview',\n",
       "     'input_price_per_token': 1e-05,\n",
       "     'output_price_per_token': 3e-05},\n",
       "    {'model': 'gpt-4-1106-preview',\n",
       "     'input_price_per_token': 1e-05,\n",
       "     'output_price_per_token': 3e-05},\n",
       "    {'model': 'gpt-4-1106-vision-preview',\n",
       "     'input_price_per_token': 1e-05,\n",
       "     'output_price_per_token': 3e-05}]},\n",
       "  {'name': 'GPT-4',\n",
       "   'variants': [{'model': 'gpt-4',\n",
       "     'input_price_per_token': 3e-05,\n",
       "     'output_price_per_token': 6e-05},\n",
       "    {'model': 'gpt-4-32k',\n",
       "     'input_price_per_token': 6e-05,\n",
       "     'output_price_per_token': 0.00012}]},\n",
       "  {'name': 'GPT-3.5 Turbo',\n",
       "   'variants': [{'model': 'gpt-3.5-turbo-0125',\n",
       "     'input_price_per_token': 5e-07,\n",
       "     'output_price_per_token': 1.5e-06},\n",
       "    {'model': 'gpt-3.5-turbo-instruct',\n",
       "     'input_price_per_token': 1.5e-06,\n",
       "     'output_price_per_token': 2e-06}]},\n",
       "  {'name': 'Embedding models',\n",
       "   'variants': [{'model': 'text-embedding-3-small',\n",
       "     'usage_price_per_token': 2e-08},\n",
       "    {'model': 'text-embedding-3-large', 'usage_price_per_token': 1.3e-07},\n",
       "    {'model': 'ada v2', 'usage_price_per_token': 1e-07}]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def load_models_config(file_path):\n",
    "    with open(file_path, \"r\") as stream:\n",
    "        try:\n",
    "            models_config = yaml.safe_load(stream)\n",
    "            return models_config\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"../config/models_config.yml\"\n",
    "models_config = load_models_config(file_path)\n",
    "models_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6c411-1beb-4f38-84dc-8ca8a4c35651",
   "metadata": {},
   "source": [
    "### Example usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee58bd65-ae68-4930-b778-3f2545879c16",
   "metadata": {},
   "source": [
    "##### Determine Expertise Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5400c7-0964-4bff-9998-9fc7c2b9bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expertise Area: Airflow, Data Engineering\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PromptPreparator class\n",
    "inference = PromptPreparator(OPENAI_CREDENTIALS, models_config)\n",
    "\n",
    "# Set the model\n",
    "inference.set_model(\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Determine the expertise area and capture usage information\n",
    "user_question = \"help me understand the Airflow concept in my files.\"\n",
    "expertise_area, usage = inference.determine_expertise_area(user_question)\n",
    "print(\"Expertise Area:\", expertise_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d7814-4535-4683-a9ef-ca81b34ad2cf",
   "metadata": {},
   "source": [
    "##### Determine Inference Price of Expertise Area Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1f0ba1-fe12-401c-b64a-49d6b937d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: 4.5499999999999995e-05\n"
     ]
    }
   ],
   "source": [
    "# Calculate the inference price based on the usage\n",
    "price = inference.calculate_inference_price(usage)\n",
    "print(f\"Price: {price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407526f5-38ea-4f9d-9cc6-be16e10b873a",
   "metadata": {},
   "source": [
    "##### Prepare the RAG Prompt with Mock Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfd36c0-bbcd-4b89-9294-39961ee7d2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in 'Airflow, Data Engineering'. A user has asked for help with the following question: 'help me understand the Airflow concept in my files.'. Please provide insights using only the information from the provided documents. If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\n",
      "\n",
      "Here are the documents provided:\n",
      "\n",
      "Document 1:\n",
      "\"\"\"\n",
      "Document 1 content about RAG...\n",
      "\"\"\"\n",
      "\n",
      "Document 2:\n",
      "\"\"\"\n",
      "Document 2 content about LLMs & RAGs...\n",
      "\"\"\"\n",
      "\n",
      "Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_documents = [\n",
    "    \"Document 1 content about RAG...\",\n",
    "    \"Document 2 content about LLMs & RAGs...\",\n",
    "]\n",
    "prompt = inference.prepare_prompt_for_llm(\n",
    "    expertise_area, user_question, context_documents\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e3476-0038-4b60-871f-67b381c1e89b",
   "metadata": {},
   "source": [
    "##### Final Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca7a974-3aed-431c-81b1-8dfa2c8f1ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in Document 1 and Document 2, it seems that the concept of Airflow is related to RAGs (Red, Amber, Green) and LLMs (Likely, Less likely, More likely). \n",
      "\n",
      "Airflow is likely being used as a tool or framework for managing and scheduling data pipelines or workflows that involve RAGs and LLMs. It is possible that Airflow is being utilized to automate the execution of tasks related to these concepts, such as data processing, analysis, or reporting.\n",
      "\n",
      "To better understand the Airflow concept in your files, you may want to look for any references to how Airflow is being used to schedule and orchestrate tasks related to RAGs and LLMs. This could include DAGs (Directed Acyclic Graphs) that represent the workflow logic, operators that define specific tasks, and connections to data sources or destinations.\n",
      "\n",
      "Overall, it appears that the use of Airflow in your files is tied to managing and executing processes related to RAGs and LLMs, potentially indicating a data engineering or workflow automation application.\n"
     ]
    }
   ],
   "source": [
    "final_response, final_usage = inference.query_openai(\n",
    "    prompt, max_completion_tokens=1500, temperature=0.7\n",
    ")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6146c-b2b5-43ab-b46e-299c457defcd",
   "metadata": {},
   "source": [
    "##### Determine Inference Price of Fianl RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "909ba6d0-ba13-4f94-bde4-6a7af9ce9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: 0.00041\n"
     ]
    }
   ],
   "source": [
    "# Calculate the inference price based on the usage\n",
    "price = inference.calculate_inference_price(final_usage)\n",
    "print(f\"Price: {price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d5466-5b46-4b1c-9074-bd8f1a9cd328",
   "metadata": {},
   "source": [
    "## Connect with me üåê\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://www.linkedin.com/in/labrijisaad/\">\n",
    "    <img src=\"https://img.shields.io/badge/LinkedIn-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn\" style=\"margin-bottom: 5px;\"/>\n",
    "  </a>\n",
    "  <a href=\"https://github.com/labrijisaad\">\n",
    "    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub\" style=\"margin-bottom: 5px;\"/>\n",
    "  </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

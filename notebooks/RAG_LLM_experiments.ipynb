{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966ddb36-32b1-45dd-a277-4da25415278f",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/labrijisaad/LLM-RAG/blob/main/notebooks/RAG_LLM_experiments.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://img.shields.io/badge/Open%20in-GitHub-blue.svg\" alt=\"Open In GitHub\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711fe42-b203-4dda-9a46-b8b18093d315",
   "metadata": {},
   "source": [
    "## <center><a><span style=\"color:red\">`OpenAI LLM` - RAG LLM Experiments</span></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7da429-fc28-47f6-a50c-b76531aff98b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"https://github.com/labrijisaad/LLM-RAG/assets/74627083/95f98e19-7675-4bd1-908d-272166856290\" style=\"width:60%;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d279c-a2e4-46e7-b49f-17aff0030915",
   "metadata": {},
   "source": [
    "### <a><span style=\"color:green\">`Class Based` - Model Inference Manager</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e04a0e2-88b0-4d8c-a2ac-95a9c1087526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import faiss\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7778c0ac-e82d-4d05-b6db-1d67cc825b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInferenceManager:\n",
    "    def __init__(self, api_key, models_config):\n",
    "        self.api_key = api_key\n",
    "        self.models_config = models_config\n",
    "        self.model = None\n",
    "        self.input_token_price = None\n",
    "        self.output_token_price = None\n",
    "\n",
    "    def set_model(self, model_name):\n",
    "        for group in self.models_config[\"models\"]:\n",
    "            for variant in group[\"variants\"]:\n",
    "                if variant[\"model\"] == model_name:\n",
    "                    self.model = model_name\n",
    "                    self.input_token_price = variant[\"input_price_per_token\"]\n",
    "                    self.output_token_price = variant[\"output_price_per_token\"]\n",
    "                    return\n",
    "        raise ValueError(f\"Model {model_name} not found in configuration.\")\n",
    "\n",
    "    def query_openai(self, prompt_text, max_completion_tokens=100, temperature=0.7):\n",
    "        if not self.model:\n",
    "            raise ValueError(\n",
    "                \"Model not set. Please use set_model() to set a model before querying.\"\n",
    "            )\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            \"max_tokens\": max_completion_tokens,\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                usage = data[\"usage\"]\n",
    "                return content, usage\n",
    "            else:\n",
    "                return (\n",
    "                    f\"HTTP Error {response.status_code}: {response.json().get('error', {}).get('message', 'An unspecified error occurred')}\",\n",
    "                    None,\n",
    "                )\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Connection error: {e}\", None\n",
    "\n",
    "    def calculate_cost(self, usage):\n",
    "        if usage:\n",
    "            total_price = (usage[\"prompt_tokens\"] * self.input_token_price) + (\n",
    "                usage[\"completion_tokens\"] * self.output_token_price\n",
    "            )\n",
    "            return total_price\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def determine_expertise_area(\n",
    "        self, user_question, max_completion_tokens, temperature\n",
    "    ):\n",
    "        prompt_text = f\"\"\"Based on the question provided, identify the relevant expertise area(s). Return your answer in the format: \n",
    "        'expertise1, expertise2, ...'. Provide only the expertise areas as a comma-separated list, no additional explanations are needed.\n",
    "        Here is the user Question:\n",
    "        {user_question}\n",
    "        \"\"\"\n",
    "        response, usage = self.query_openai(\n",
    "            prompt_text, max_completion_tokens, temperature\n",
    "        )\n",
    "        return response.strip(), (\n",
    "            usage if response else \"Error determining expertise area.\"\n",
    "        )\n",
    "\n",
    "    def prepare_prompt_for_llm(self, expertise_area, user_question, context_documents):\n",
    "        prompt = (\n",
    "            f\"You are an expert in '{expertise_area}'. A user has asked for help with the following question: \"\n",
    "            f\"'{user_question}'. Please provide insights using only the information from the provided documents. \"\n",
    "            \"If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\\n\\n\"\n",
    "            \"Here are the documents provided:\\n\\n\"\n",
    "        )\n",
    "        for i, document in enumerate(context_documents, start=1):\n",
    "            prompt += f'Document {i}:\\n\"\"\"\\n{document}\\n\"\"\"\\n\\n'\n",
    "        prompt += \"Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\\n\\n\"\n",
    "        prompt += \"Start your response below:\\n\\n\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "# Load Model Configs function\n",
    "def load_models_config(config_file_path):\n",
    "    with open(config_file_path, \"r\") as config_file:\n",
    "        try:\n",
    "            models_config = yaml.safe_load(config_file)\n",
    "            return models_config\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822faaa8-c510-4ac4-944d-141cd1374333",
   "metadata": {},
   "source": [
    "### <a><span style=\"color:green\">`Class Based` - Text Vectorization</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d45b82a-8f5d-4210-ba90-f171982d557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticVectorizer:\n",
    "    def __init__(self, api_key, models_config):\n",
    "        self.api_key = api_key\n",
    "        self.models_config = models_config\n",
    "        self.model = None\n",
    "        self.usage_price_per_token = 0\n",
    "        self.texts = []\n",
    "        self.embeddings = []\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def set_model(self, model_name):\n",
    "        found = False\n",
    "        for group in self.models_config[\"models\"]:\n",
    "            for variant in group[\"variants\"]:\n",
    "                if variant[\"model\"] == model_name:\n",
    "                    self.model = model_name\n",
    "                    self.usage_price_per_token = variant.get(\"usage_price_per_token\", 0)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            raise ValueError(f\"Model {model_name} not found in configuration.\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses the text before embedding.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        return text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    def read_and_process_markdown(self, file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        sections = re.split(r\"\\n(#{1,3} .*)\\n\", text)\n",
    "        self.texts = [self.preprocess_text(sections[0])] + [\n",
    "            self.preprocess_text(f\"{sections[i]}\\n{sections[i + 1]}\")\n",
    "            for i in range(1, len(sections), 2)\n",
    "        ]\n",
    "        return self.texts\n",
    "\n",
    "    def query_openai_embedding(self, text):\n",
    "        preprocessed_text = self.preprocess_text(text)\n",
    "        url = f\"https://api.openai.com/v1/embeddings\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        payload = {\n",
    "            \"input\": preprocessed_text,\n",
    "            \"model\": self.model,\n",
    "        }\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            embedding = np.array(data[\"data\"][0][\"embedding\"], dtype=\"float32\")\n",
    "            return embedding, data.get(\"usage\", {})\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to generate embedding: Status code {response.status_code}, Response: {response.text}\"\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "    def generate_embeddings(self, save_index=False, index_path=None):\n",
    "        total_cost = 0\n",
    "        for text in tqdm(self.texts):\n",
    "            embedding, usage = self.query_openai_embedding(text)\n",
    "            if embedding is not None:\n",
    "                self.embeddings.append(embedding)\n",
    "                total_cost += self.calculate_cost(usage)\n",
    "        self.embeddings = np.array(self.embeddings)\n",
    "        self.create_faiss_index()\n",
    "\n",
    "        if save_index and index_path:\n",
    "            self.save_faiss_index(index_path)\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    def create_faiss_index(self):\n",
    "        if self.embeddings.size > 0:\n",
    "            dimension = self.embeddings.shape[1]\n",
    "            self.faiss_index = faiss.IndexFlatL2(dimension)\n",
    "            self.faiss_index.add(self.embeddings)\n",
    "        else:\n",
    "            print(\"No embeddings to add to FAISS index.\")\n",
    "\n",
    "    def search_similar_sections(self, query_text, num_results):\n",
    "        query_embedding, _ = self.query_openai_embedding(query_text)\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS index is not initialized. Please create the index before searching.\"\n",
    "            )\n",
    "        if query_embedding is None:\n",
    "            return []\n",
    "        distances, indices = self.faiss_index.search(\n",
    "            np.array([query_embedding], dtype=\"float32\"), num_results\n",
    "        )\n",
    "        return [self.texts[idx] for idx in indices[0] if idx < len(self.texts)]\n",
    "\n",
    "    def save_faiss_index(self, index_path):\n",
    "        if self.faiss_index:\n",
    "            faiss.write_index(self.faiss_index, index_path)\n",
    "        else:\n",
    "            print(\"FAISS index is not initialized.\")\n",
    "\n",
    "    def load_faiss_index(self, index_path):\n",
    "        self.faiss_index = faiss.read_index(index_path)\n",
    "\n",
    "    def calculate_cost(self, usage):\n",
    "        total_tokens = usage.get(\"total_tokens\", 0)\n",
    "        total_price = total_tokens * self.usage_price_per_token\n",
    "        return total_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3a2dc-11e9-4573-9489-3dc93b5660d9",
   "metadata": {},
   "source": [
    "### <a><span style=\"color:green\">`Class Based` - LLM RAG Pipeline</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2bd701-a1ac-405b-aca3-9b74efe14f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPipeline:\n",
    "    def __init__(self, openai_api_key, models_config):\n",
    "        self.embedder = SemanticVectorizer(openai_api_key, models_config)\n",
    "        self.model_inference_manager = ModelInferenceManager(\n",
    "            openai_api_key, models_config\n",
    "        )\n",
    "\n",
    "    def setup_semantic_database(\n",
    "        self, markdown_path, embedding_model, save_index=False, index_path=None\n",
    "    ):\n",
    "        self.embedder.set_model(embedding_model)\n",
    "        self.embedder.read_and_process_markdown(markdown_path)\n",
    "        total_cost = self.embedder.generate_embeddings(\n",
    "            save_index=save_index, index_path=index_path\n",
    "        )\n",
    "        return total_cost\n",
    "\n",
    "    def find_similar_documents(self, query_text, num_results):\n",
    "        similar_docs = self.embedder.search_similar_sections(query_text, num_results)\n",
    "        return similar_docs\n",
    "\n",
    "    def determine_expertise_and_prepare_prompt(\n",
    "        self,\n",
    "        user_query,\n",
    "        similar_docs,\n",
    "        inference_model,\n",
    "        max_completion_tokens,\n",
    "        temperature,\n",
    "    ):\n",
    "        self.model_inference_manager.set_model(inference_model)\n",
    "        identified_expertise_area, expertise_area_usage = (\n",
    "            self.model_inference_manager.determine_expertise_area(\n",
    "                user_query, max_completion_tokens, temperature\n",
    "            )\n",
    "        )\n",
    "        # Calculate the cost for determining expertise area\n",
    "        expertise_area_cost = 0  # Default to 0\n",
    "        if expertise_area_usage:\n",
    "            expertise_area_cost = self.model_inference_manager.calculate_cost(\n",
    "                expertise_area_usage\n",
    "            )\n",
    "        context_enhanced_prompt = self.model_inference_manager.prepare_prompt_for_llm(\n",
    "            identified_expertise_area, user_query, similar_docs\n",
    "        )\n",
    "        return context_enhanced_prompt, expertise_area_cost\n",
    "\n",
    "    def query_model_for_response(\n",
    "        self, context_enhanced_prompt, max_completion_tokens, temperature\n",
    "    ):\n",
    "        contextual_response, response_usage = self.model_inference_manager.query_openai(\n",
    "            context_enhanced_prompt, max_completion_tokens, temperature\n",
    "        )\n",
    "        # Calculate the cost for querying the model\n",
    "        response_cost = 0  # Default to 0\n",
    "        if response_usage:\n",
    "            response_cost = self.model_inference_manager.calculate_cost(response_usage)\n",
    "\n",
    "        return contextual_response, response_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f1914-a383-4d0f-93ed-4a3c8a4075f3",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"https://github.com/labrijisaad/LLM-RAG/assets/74627083/ca509e9c-9c75-44c3-965a-94e1d7106412\" style=\"width:35%;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41191c-81b9-49db-b1b6-ea16be1d33ee",
   "metadata": {},
   "source": [
    "#### Loading `OpenAI Key` & `Model Configuration` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb8e35d-9ca8-4457-9e5f-be2d18df3784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['models'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Laod OpenAI Key\n",
    "with open(\"../secrets/credentials.yml\", \"r\") as credentials_file:\n",
    "    config = yaml.safe_load(credentials_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_CREDENTIALS\"]\n",
    "\n",
    "# Load Models Config\n",
    "MODELS_CONFIG = load_models_config(\"../config/models_config.yml\")\n",
    "MODELS_CONFIG.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e79d2-12a7-4eef-826f-194fbc9a55ff",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44264662-38d2-4a53-a0d8-7ee1d22c87ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df37cba2afd44c2bb3962cbfeda0f20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for setting up the semantic database: $3.188000000000001e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize the QueryPipeline\n",
    "query_pipeline = QueryPipeline(OPENAI_API_KEY, MODELS_CONFIG)\n",
    "\n",
    "# Set up the semantic database\n",
    "total_cost = query_pipeline.setup_semantic_database(\n",
    "    markdown_path=\"../data/raw/mock_markdown.md\",\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "print(f\"Total cost for setting up the semantic database: ${total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b7930-469d-4267-85d9-07fac83cf83e",
   "metadata": {},
   "source": [
    "###### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4ee7ab-2df7-4c53-9e60-4c35ac7a9fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for determining expertise area: $4.4e-05\n",
      "Cost for querying the model for a response: $0.000527\n",
      "--------\n",
      "You are an expert in 'Healthcare, Data Analysis'. A user has asked for help with the following question: 'Where in these files is Health??'. Please provide insights using only the information from the provided documents. If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\n",
      "\n",
      "Here are the documents provided:\n",
      "\n",
      "Document 1:\n",
      "\"\"\"\n",
      "# comprehensive guide on health and fitness  health and fitness have become central to modern lifestyle, emphasizing the importance of regular exercise, balanced diet, and mental well-being.\n",
      "\"\"\"\n",
      "\n",
      "Document 2:\n",
      "\"\"\"\n",
      "## healthcare  in healthcare, ai is being used to make more accurate diagnoses, predict patient outcomes, and personalize patient treatment plans.\n",
      "\"\"\"\n",
      "\n",
      "Document 3:\n",
      "\"\"\"\n",
      "### types of physical activities  - **aerobic exercises**: enhance heart and lung health and improve stamina. - **strength training**: builds muscle strength and improves metabolism. - **flexibility exercises**: increases range of motion and reduces injury risks.\n",
      "\"\"\"\n",
      "\n",
      "Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n",
      "--------\n",
      "Based on the information provided in the documents, the concept of health is mentioned in Document 1 and Document 2. \n",
      "\n",
      "In Document 1, health is discussed in the context of a comprehensive guide on health and fitness. It highlights the importance of regular exercise, balanced diet, and mental well-being in modern lifestyle. This indicates that health is associated with physical fitness, nutrition, and mental wellness.\n",
      "\n",
      "In Document 2, healthcare is mentioned in the context of using artificial intelligence (AI) to improve healthcare practices. AI is being utilized to make more accurate diagnoses, predict patient outcomes, and personalize treatment plans. While not explicitly mentioned, the use of AI in healthcare suggests a focus on improving patient health outcomes and providing personalized care.\n",
      "\n",
      "Although the term \"health\" is not explicitly mentioned in Document 3, the discussion on different types of physical activities such as aerobic exercises, strength training, and flexibility exercises indirectly relates to health. These activities are known to have various health benefits, including enhancing heart and lung health, improving muscle strength, and reducing the risk of injuries.\n",
      "\n",
      "Overall, the concept of health is implicitly present in all three documents through discussions on fitness, healthcare practices, and physical activities that contribute to overall well-being. Health is a central theme in these documents, emphasizing the importance of physical, mental, and emotional well-being in maintaining a healthy lifestyle.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Find similar documents\n",
    "query_text = \"Where in these files is Health??\"\n",
    "similar_docs = query_pipeline.find_similar_documents(\n",
    "    query_text=query_text, num_results=3\n",
    ")\n",
    "\n",
    "# Step 3: Determine expertise area and prepare the prompt\n",
    "context_enhanced_prompt, expertise_area_cost = (\n",
    "    query_pipeline.determine_expertise_and_prepare_prompt(\n",
    "        user_query=query_text,\n",
    "        similar_docs=similar_docs,\n",
    "        inference_model=\"gpt-3.5-turbo-0125\",\n",
    "        max_completion_tokens=150,\n",
    "        temperature=0.2,\n",
    "    )\n",
    ")\n",
    "print(f\"Cost for determining expertise area: ${expertise_area_cost}\")\n",
    "\n",
    "# Step 4: Query the model for a response\n",
    "contextual_response, response_cost = query_pipeline.query_model_for_response(\n",
    "    context_enhanced_prompt=context_enhanced_prompt,\n",
    "    max_completion_tokens=1500,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(f\"Cost for querying the model for a response: ${response_cost}\")\n",
    "\n",
    "print(\"--------\")\n",
    "print(context_enhanced_prompt)\n",
    "print(\"--------\")\n",
    "print(contextual_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c767278-f25e-40af-89fc-272b08b77b68",
   "metadata": {},
   "source": [
    "###### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe879c6c-c7d5-4f7f-bc67-f532a9e4ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for determining expertise area: $4.55e-05\n",
      "Cost for querying the model for a response: $0.0004895\n",
      "--------\n",
      "You are an expert in 'Artificial Intelligence, Data Analysis'. A user has asked for help with the following question: 'Where in these files is AI ??'. Please provide insights using only the information from the provided documents. If certain aspects are ambiguous or the documents do not fully address the question, please make educated inferences based on your expertise.\n",
      "\n",
      "Here are the documents provided:\n",
      "\n",
      "Document 1:\n",
      "\"\"\"\n",
      "# introduction to ai  artificial intelligence (ai) has rapidly become a key technology in many industries, revolutionizing processes and efficiency.\n",
      "\"\"\"\n",
      "\n",
      "Document 2:\n",
      "\"\"\"\n",
      "# applications of ai  ai technology is not just a scientific discipline, but a driver of high-tech innovation and real-world applications.\n",
      "\"\"\"\n",
      "\n",
      "Document 3:\n",
      "\"\"\"\n",
      "## history of ai  the concept of artificial intelligence has been around for centuries, but it wasn't until the 20th century that it became a field of study. alan turing, a british mathematician and logician, laid the groundwork for modern computing and theorized about machines that could think.\n",
      "\"\"\"\n",
      "\n",
      "Given your expertise and the information provided in these documents, synthesize the key insights to craft a detailed and relevant response to the above question.\n",
      "\n",
      "Start your response below:\n",
      "\n",
      "\n",
      "--------\n",
      "Based on the information provided in the documents, it is clear that artificial intelligence (AI) is a prominent topic discussed throughout. In Document 1, it is mentioned that AI has rapidly become a key technology in many industries, indicating its widespread adoption and impact. Document 2 further emphasizes the applications of AI, stating that it is not just a scientific discipline but also a driver of high-tech innovation and real-world applications. This suggests that AI is actively being used in various fields to drive innovation and improve efficiency.\n",
      "\n",
      "Additionally, Document 3 delves into the history of AI, mentioning that the concept has been around for centuries but gained significant traction in the 20th century with the work of Alan Turing. Turing, a British mathematician and logician, laid the groundwork for modern computing and theorized about machines that could think, highlighting the foundational work that has paved the way for the development of AI technologies today.\n",
      "\n",
      "Based on these insights, it can be inferred that AI is a significant focus across these documents, with mentions of its evolution, applications, and historical context. Therefore, it is safe to say that AI is a central theme in these files, with discussions ranging from its impact on industries to its historical development.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Find similar documents\n",
    "query_text = \"Where in these files is AI ??\"\n",
    "similar_docs = query_pipeline.find_similar_documents(\n",
    "    query_text=query_text, num_results=3\n",
    ")\n",
    "\n",
    "# Step 3: Determine expertise area and prepare the prompt\n",
    "context_enhanced_prompt, expertise_area_cost = (\n",
    "    query_pipeline.determine_expertise_and_prepare_prompt(\n",
    "        user_query=query_text,\n",
    "        similar_docs=similar_docs,\n",
    "        inference_model=\"gpt-3.5-turbo-0125\",\n",
    "        max_completion_tokens=150,\n",
    "        temperature=0.2,\n",
    "    )\n",
    ")\n",
    "print(f\"Cost for determining expertise area: ${expertise_area_cost}\")\n",
    "\n",
    "# Step 4: Query the model for a response\n",
    "contextual_response, response_cost = query_pipeline.query_model_for_response(\n",
    "    context_enhanced_prompt=context_enhanced_prompt,\n",
    "    max_completion_tokens=1500,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(f\"Cost for querying the model for a response: ${response_cost}\")\n",
    "\n",
    "print(\"--------\")\n",
    "print(context_enhanced_prompt)\n",
    "print(\"--------\")\n",
    "print(contextual_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e7bfd-84ab-45a9-bd30-e5437a60f913",
   "metadata": {},
   "source": [
    "## Connect with me 🌐\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://www.linkedin.com/in/labrijisaad/\">\n",
    "    <img src=\"https://img.shields.io/badge/LinkedIn-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn\" style=\"margin-bottom: 5px;\"/>\n",
    "  </a>\n",
    "  <a href=\"https://github.com/labrijisaad\">\n",
    "    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub\" style=\"margin-bottom: 5px;\"/>\n",
    "  </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

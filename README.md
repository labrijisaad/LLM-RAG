# `LLM RAG` - Streamlit RAG Language Model App ğŸ¤–

## ğŸŒŸ Overview 
This is a Streamlit app leveraging a RAG (Retrieval-Augmented Generation) Language Model (LLM) with FAISS to offer answers from uploaded markdown files ğŸ“‚. The app allows users to upload files, ask questions related to the content of these files, and receive relevant answers generated by the RAG LLM ğŸ“š.

## ğŸ› ï¸ System Architecture
The following diagram illustrates the flow of data through the system:

```mermaid
graph TD
    A[User Files] -->|Read & Process| B[Semantic Database Setup]
    B -->|Generate Embeddings & FAISS Index| C[Vector Store]
    C -->|Utilize OpenAI's Models| D[Semantic Search]
    E[User Query] -->|Vectorization| D
    D -->|Select Top Documents| F[Top Documents]
    F -->|Include Selected Docs in Context| G[Contextualized Documents]
    E -->|Determine Expertise using OpenAI| H[Expertise Area]
    H -->|Formulate Prompt| I[Prompt with Context]
    G --> I
    I -->|Query OpenAI LLM| J[LLM Response]
    J -->|Generate Answer| K[Answer]

    style A fill:#7f7f7f,stroke:#fff,stroke-width:2px
    style B fill:#8fa1b3,stroke:#fff,stroke-width:2px
    style C fill:#8fa1b3,stroke:#fff,stroke-width:2px
    style D fill:#8fa1b3,stroke:#fff,stroke-width:2px
    style E fill:#7f7f7f,stroke:#fff,stroke-width:2px
    style F fill:#8fa1b3,stroke:#fff,stroke-width:2px
    style G fill:#8fa1b3,stroke:#fff,stroke-width:2px
    style H fill:#8fa1b3,stroke:#fff,stroke-width:2px
    style I fill:#e07b53,stroke:#fff,stroke-width:2px
    style J fill:#e07b53,stroke:#fff,stroke-width:2px
    style K fill:#e07b53,stroke:#fff,stroke-width:2px
```

## Project Structure ğŸ—ï¸
The project's main directories are laid out as follows:

```
LLM-RAG/
â”œâ”€â”€ .github/workflows/          # CI/CD pipeline definitions
â”œâ”€â”€ configs/                    # Configuration files for the model (model names, pricing..)
â”œâ”€â”€ data/                       # Data and indices used by the app (FAISS Knowledge Base)
â”œâ”€â”€ docker/                     # Docker related files 
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for experiments
â”œâ”€â”€ secrets/                    # API keys and other secrets (excluded from version control)
â”œâ”€â”€ src/                        # Source code for the LLM RAG logic
â”œâ”€â”€ streamlit_app/              # Streamlit app files for the Web Interface
â”œâ”€â”€ tests/                      # Test cases for the application
â”œâ”€â”€ .dockerignore               # Specifies ignored files in Docker builds
â”œâ”€â”€ .gitignore                  # Specifies untracked files ignored by git
â”œâ”€â”€ Dockerfile                  # Dockerfile for building the Docker image
â”œâ”€â”€ Makefile                    # Make commands for building and running the app ğŸ§‘â€ğŸ’»
â”œâ”€â”€ README.md                   # Documentation and instructions
â”œâ”€â”€ requirements.txt            # Python dependencies for the project
â””â”€â”€ (additional project files and scripts)
```

## ğŸš€ Getting Started

To begin using the LLM RAG app, follow these simple steps:

1. **Clone the Repository:**
   ```
   git clone https://github.com/labrijisaad/LLM-RAG.git
   ```

2. **Create the Environment:**
   Set up your virtual environment using either venv or conda:
   ```
   # Using venv
   python -m venv env
   source env/bin/activate
   
   # Using conda
   conda create --name env_name
   conda activate env_name
   ```

3. **Install Dependencies:**
   Install the required dependencies by

 running:
   ```
   pip install -r requirements.txt
   ```

4. **Set Up OpenAI API:**
   Rename the example credentials file to `secrets/credentials.yml` and replace the placeholder key ('sk-xxx') with your actual OpenAI API key. You can obtain your API key by following the instructions provided in the [OpenAI documentation](https://platform.openai.com/docs/quickstart?context=python).
   ```
   rename secrets/credentials-example.yml secrets/credentials.yml
   ```

5. **Run the Streamlit App:**
   Launch the Streamlit app using either the provided Makefile command or directly via the Streamlit CLI:
   ```
   # Using Makefile
   make stream
   
   # Or directly
   streamlit run streamlit_app/main.py
   ```
## ğŸ³ Docker Version
The application is available as a Docker container. To set up the Docker environment:

1. **Build the Docker Image:**
   Run `make docker-build` or use the Docker command directly:
   ```shell
   docker build -t llm_rag_app -f docker/Dockerfile .
   ```

2. **Run the Docker Container:**
   Start the container with `make docker-run` or use the Docker command:
   ```shell
   docker run -p 8501:8501 -v $(pwd)/secrets:/app/secrets llm_rag_app
   ```

3. **Stop the Docker Container:**
   Use `make docker-kill` to stop and remove the container.

The Streamlit app will be available at **`http://localhost:8501`**.


## ğŸŒ Connect with me
<div align="center">
  <a href="https://www.linkedin.com/in/labrijisaad/">
    <img src="https://img.shields.io/badge/LinkedIn-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" style="margin-bottom: 5px;"/>
  </a>
  <a href="https://github.com/labrijisaad">
    <img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" alt="GitHub" style="margin-bottom: 5px;"/>
  </a>
</div>
